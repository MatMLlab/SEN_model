"""
Capsule autoencoder implementation.
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import sonnet as snt
import tensorflow.compat.v1 as tf
import tensorflow_probability as tfp

from models.layers import ocae_block as _capsule
import geo_block
from Loss import pred_loss, pred_loss_1
from models.layers.squash_block import Model

tfd = tfp.distributions


class MaterialCapsule(snt.AbstractModule):
  """ OCAE Capsule decoder for constellations."""

  def __init__(self, n_caps, n_caps_dims, n_votes, **capsule_kwargs):
    """Builds the module.
    Args:
      n_caps: int, number of capsules.
      n_caps_dims: int, number of capsule coordinates.
      n_votes: int, number of votes generated by each capsule.
      **capsule_kwargs: kwargs passed to capsule layer.
    """
    super(MaterialCapsule, self).__init__()
    self._n_caps = n_caps
    self._n_caps_dims = n_caps_dims
    self._n_votes = n_votes
    self._capsule_kwargs = capsule_kwargs

  def _build(self, h, x, presence=None):
    """Builds the module.
    Args:
      h: Tensor of encodings of shape [B, n_enc_dims].
      x: Tensor of inputs of shape [B, n_points, n_input_dims]
      presence: Tensor of shape [B, n_points, 1] or None; if it exists, it
        indicates which input points exist.
    Returns:
      A bunch of stuff.
    """
    batch_size = tf.shape(x)[0]

    capsule = _capsule.CapsuleLayer(self._n_caps, self._n_caps_dims,
                                    self._n_votes, **self._capsule_kwargs)

    res = capsule(h)
    vote_shape = [batch_size, self._n_caps, self._n_votes, 6]
    res.vote = tf.reshape(res.vote[Ellipsis, :-1, :], vote_shape)

    votes, scale, vote_presence_prob, \
    raw_caps_params = res.vote, res.scale, res.vote_presence, res.raw_caps_params

    likelihood = _capsule.CapsuleLikelihood(votes, scale, vote_presence_prob, raw_caps_params)
    ll_res = likelihood(x,
                        pres_logit_per_vote = res.pres_logit_per_vote,
                        pres_logit_per_caps = res.pres_logit_per_caps,
                        presence = presence)
    res.update(ll_res._asdict())

    caps_presence_prob = tf.reduce_max(
        tf.reshape(vote_presence_prob,
                   [batch_size, self._n_caps, self._n_votes]), 2, name = 'caps_presence_prob')

    res.caps_presence_prob = caps_presence_prob

    return res, caps_presence_prob


class MaterialAutoencoder(Model):
  """Capsule autoencoder."""

  OutputTuple = collections.namedtuple('MaterialAutoencoder',
                                       ('rec_ll '
                                        'log_prob '
                                        'dynamic_weights_l2 '
                                        'primary_caps_l1 '
                                        'posterior_within_sparsity_loss '
                                        'posterior_between_sparsity_loss '
                                        'prior_within_sparsity_loss '
                                        'prior_between_sparsity_loss '
                                        'weight_decay_loss '
                                        'posterior_cls_xe '
                                        'prior_cls_xe '
                                        'best_pre_loss '
                                        'votes '
                                        'mass_explained_by_capsule '
                                        'caps_presence_prob '))

  def __init__(
      self,
      primary_encoder,
      primary_decoder,
      encoder,
      decoder,
      input_key,
      vote_type='soft',
      pres_type='soft',
      stop_grad_caps_inpt = True,
      stop_grad_caps_target = True,
      feed_templates = True,
      label_key=None,
      n_classes=None,
      dynamic_l2_weight=0.,
      caps_ll_weight=0.,
      img_summaries=False,
      prior_sparsity_loss_type='kl',
      prior_within_example_sparsity_weight=0.,
      prior_between_example_sparsity_weight=0.,
      prior_within_example_constant=0.,
      posterior_sparsity_loss_type='kl',
      posterior_within_example_sparsity_weight=0.,
      posterior_between_example_sparsity_weight=0.,
      primary_caps_sparsity_weight=0.5,
      weight_decay=0.,
          k_eig=0,
      prep='none',):

    super(MaterialAutoencoder, self).__init__()
    self._stop_grad_caps_inpt = stop_grad_caps_inpt
    self._stop_grad_caps_target = stop_grad_caps_target
    self._feed_templates = feed_templates
    self._primary_encoder = primary_encoder
    self._primary_decoder = primary_decoder
    self._encoder = encoder
    self._decoder = decoder
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._input_key = input_key
    self._label_key = label_key
    self._n_classes = n_classes
    self.k_eig = k_eig

    self._dynamic_l2_weight = dynamic_l2_weight
    self._caps_ll_weight = caps_ll_weight
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._img_summaries = img_summaries
    self._prior_sparsity_loss_type = prior_sparsity_loss_type
    self._prior_within_example_sparsity_weight = prior_within_example_sparsity_weight
    self._prior_between_example_sparsity_weight = prior_between_example_sparsity_weight
    self._prior_within_example_constant = prior_within_example_constant
    self._posterior_sparsity_loss_type = posterior_sparsity_loss_type
    self._posterior_within_example_sparsity_weight = posterior_within_example_sparsity_weight
    self._posterior_between_example_sparsity_weight = posterior_between_example_sparsity_weight
    self._primary_caps_sparsity_weight = primary_caps_sparsity_weight
    self._weight_decay = weight_decay

    self._prep = prep

  def _build(self, final_vec, real_bg):

    input_x = tf.expand_dims(final_vec, 0) # 1, N, 256
    batch_size = tf.shape(input_x)[1]

    #pioir_pre = tf.reshape(out, (batch_size, 1))

    #Part Capsule Autoencoder
    # Multi-dimensional output
    primary_caps = self._primary_encoder(input_x)

    # Presence
    pres = primary_caps.presence

    expanded_pres = tf.expand_dims(pres, -1)

    # pose dimension of capsule
    pose = primary_caps.pose
    pose_in = tf.expand_dims(pose, -1, name = 'pose_in')
    pose_out = snt.Conv2D(9, 2, 1, name = 'pose_2')(pose_in)

    pose_out = snt.Conv2D(1, 2, 1, name = 'pose_3')(pose_out)

    dense_out = tf.squeeze(pose_out, -1, name = 'dense_out')


    # make input data for Part Capsule (PC) decoder
    input_pose = tf.concat([pose, 1. - expanded_pres], -1)
    input_pres = pres

    pose_fea = tf.concat([dense_out, primary_caps.feature], -1)
    pose_pre = snt.BatchApply(snt.Linear(1, name = 'Linear_pose'))(pose_fea)

    #pose_pre = tf.multiply(pose_pre, expanded_pres)
    #fea_pre = tf.nn.dropout(fea_pre, rate = 0.3)
    fea_pres = tf.multiply(pose_pre, expanded_pres, name = 'multiply_fea')

    pose_fea = tf.concat([tf.squeeze(pose_pre, -1), tf.squeeze(fea_pres, -1)], axis=-1)
    pose_fea = tf.concat([pose_fea, final_vec], axis=-1)
    dyn_pre = snt.Linear(64)(pose_fea)
    dyn_pre = snt.Linear(1)(dyn_pre)
    
    mae = pred_loss_1(dyn_pre, real_bg)

    likelihood_loss = tf.stack([mae,      mae], name = 'likelihood_loss')
    mse_loss        = tf.stack([mae,           mae], name = 'mse_loss')
    rec_mse_loss    = tf.stack([mae,       mae], name = 'rec_mse_loss')

    return likelihood_loss, mse_loss, rec_mse_loss, dyn_pre


  def _likelihood_loss(self, data, res):
      likelihood_loss, mse = tf.unstack(res)

      loss = likelihood_loss

      phi = 0.1
      try:
          loss = phi * loss
          # loss += posterior_pre_xe + prior_pre_xe
      except AttributeError:
          pass

      return loss

  def _mse_loss(self, data, res):
      rec_mse, mse = tf.unstack(res)

      alpha = 1
      loss = alpha * mse
      return loss

  def _rec_mse_loss(self, data, res):
      rec_mse, mse = tf.unstack(res)

      beta = 1
      loss = beta * rec_mse
      return loss

  def _vamp_aut_loss(self, data, res):
      vamp_aut_loss, mse = tf.unstack(res)

      gamma = 1
      loss = gamma * vamp_aut_loss
      return loss

  def _vamp_sym_loss(self, data, res):
      vamp_sym_loss, mse = tf.unstack(res)

      lambd = 1
      loss = lambd * vamp_sym_loss
      return loss









