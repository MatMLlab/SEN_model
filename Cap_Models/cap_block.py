"""
Capsule autoencoder implementation.
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import sonnet as snt
import tensorflow.compat.v1 as tf
import tensorflow_probability as tfp

from models.layers import ocae_block as _capsule
import geo_block
from Loss import pred_loss, pred_loss_1
from models.layers.squash_block import Model

tfd = tfp.distributions


class MaterialCapsule(snt.AbstractModule):
  """ OCAE Capsule decoder for constellations."""

  def __init__(self, n_caps, n_caps_dims, n_votes, **capsule_kwargs):
    """Builds the module.
    Args:
      n_caps: int, number of capsules.
      n_caps_dims: int, number of capsule coordinates.
      n_votes: int, number of votes generated by each capsule.
      **capsule_kwargs: kwargs passed to capsule layer.
    """
    super(MaterialCapsule, self).__init__()
    self._n_caps = n_caps
    self._n_caps_dims = n_caps_dims
    self._n_votes = n_votes
    self._capsule_kwargs = capsule_kwargs

  def _build(self, h, x, presence=None):
    """Builds the module.
    Args:
      h: Tensor of encodings of shape [B, n_enc_dims].
      x: Tensor of inputs of shape [B, n_points, n_input_dims]
      presence: Tensor of shape [B, n_points, 1] or None; if it exists, it
        indicates which input points exist.
    Returns:
      A bunch of stuff.
    """
    batch_size = tf.shape(x)[0]

    capsule = _capsule.CapsuleLayer(self._n_caps, self._n_caps_dims,
                                    self._n_votes, **self._capsule_kwargs)

    res = capsule(h)
    vote_shape = [batch_size, self._n_caps, self._n_votes, 6]
    res.vote = tf.reshape(res.vote[Ellipsis, :-1, :], vote_shape)

    votes, scale, vote_presence_prob, \
    raw_caps_params = res.vote, res.scale, res.vote_presence, res.raw_caps_params

    likelihood = _capsule.CapsuleLikelihood(votes, scale, vote_presence_prob, raw_caps_params)
    ll_res = likelihood(x,
                        pres_logit_per_vote = res.pres_logit_per_vote,
                        pres_logit_per_caps = res.pres_logit_per_caps,
                        presence = presence)
    res.update(ll_res._asdict())

    caps_presence_prob = tf.reduce_max(
        tf.reshape(vote_presence_prob,
                   [batch_size, self._n_caps, self._n_votes]), 2, name = 'caps_presence_prob')

    res.caps_presence_prob = caps_presence_prob

    return res, caps_presence_prob


class MaterialAutoencoder(Model):
  """Capsule autoencoder."""

  OutputTuple = collections.namedtuple('MaterialAutoencoder',
                                       ('rec_ll '
                                        'log_prob '
                                        'dynamic_weights_l2 '
                                        'primary_caps_l1 '
                                        'posterior_within_sparsity_loss '
                                        'posterior_between_sparsity_loss '
                                        'prior_within_sparsity_loss '
                                        'prior_between_sparsity_loss '
                                        'weight_decay_loss '
                                        'posterior_cls_xe '
                                        'prior_cls_xe '
                                        'best_pre_loss '
                                        'votes '
                                        'mass_explained_by_capsule '
                                        'caps_presence_prob '))

  def __init__(
      self,
      primary_encoder,
      primary_decoder,
      encoder,
      decoder,
      input_key,
      vote_type='soft',
      pres_type='soft',
      stop_grad_caps_inpt = True,
      stop_grad_caps_target = True,
      feed_templates = True,
      label_key=None,
      n_classes=None,
      dynamic_l2_weight=0.,
      caps_ll_weight=0.,
      img_summaries=False,
      prior_sparsity_loss_type='kl',
      prior_within_example_sparsity_weight=0.,
      prior_between_example_sparsity_weight=0.,
      prior_within_example_constant=0.,
      posterior_sparsity_loss_type='kl',
      posterior_within_example_sparsity_weight=0.,
      posterior_between_example_sparsity_weight=0.,
      primary_caps_sparsity_weight=0.5,
      weight_decay=0.,
          k_eig=0,
      prep='none',):

    super(MaterialAutoencoder, self).__init__()
    self._stop_grad_caps_inpt = stop_grad_caps_inpt
    self._stop_grad_caps_target = stop_grad_caps_target
    self._feed_templates = feed_templates
    self._primary_encoder = primary_encoder
    self._primary_decoder = primary_decoder
    self._encoder = encoder
    self._decoder = decoder
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._input_key = input_key
    self._label_key = label_key
    self._n_classes = n_classes
    self.k_eig = k_eig

    self._dynamic_l2_weight = dynamic_l2_weight
    self._caps_ll_weight = caps_ll_weight
    self._vote_type = vote_type
    self._pres_type = pres_type
    self._img_summaries = img_summaries
    self._prior_sparsity_loss_type = prior_sparsity_loss_type
    self._prior_within_example_sparsity_weight = prior_within_example_sparsity_weight
    self._prior_between_example_sparsity_weight = prior_between_example_sparsity_weight
    self._prior_within_example_constant = prior_within_example_constant
    self._posterior_sparsity_loss_type = posterior_sparsity_loss_type
    self._posterior_within_example_sparsity_weight = posterior_within_example_sparsity_weight
    self._posterior_between_example_sparsity_weight = posterior_between_example_sparsity_weight
    self._primary_caps_sparsity_weight = primary_caps_sparsity_weight
    self._weight_decay = weight_decay

    self._prep = prep

  def _build(self, final_vec, real_bg):

    input_x = tf.expand_dims(final_vec, 0) # 1, N, 256
    batch_size = tf.shape(input_x)[1]

    #pioir_pre = tf.reshape(out, (batch_size, 1))

    #Part Capsule Autoencoder
    # Multi-dimensional output
    primary_caps = self._primary_encoder(input_x)

    # Presence
    pres = primary_caps.presence

    expanded_pres = tf.expand_dims(pres, -1)

    # pose dimension of capsule
    pose = primary_caps.pose
    pose_in = tf.expand_dims(pose, -1, name = 'pose_in')
    pose_out = snt.Conv2D(16, 2, 1, name = 'pose_2')(pose_in)

    pose_out = snt.Conv2D(1, 2, 1, name = 'pose_3')(pose_out)

    dense_out = tf.squeeze(pose_out, -1, name = 'dense_out')


    # make input data for Part Capsule (PC) decoder
    input_pose = tf.concat([pose, 1. - expanded_pres], -1)
    input_pres = pres

    pose_fea = tf.concat([dense_out, primary_caps.feature], -1)
    pose_pre = snt.BatchApply(snt.Linear(1, name = 'Linear_pose'))(pose_fea)

    #pose_pre = tf.multiply(pose_pre, expanded_pres)
    #fea_pre = tf.nn.dropout(fea_pre, rate = 0.3)
    fea_pres = tf.multiply(pose_pre, expanded_pres, name = 'multiply_fea')

    pose_fea = tf.concat([tf.squeeze(pose_pre, -1), tf.squeeze(fea_pres, -1)], axis=-1)
    dyn_pre = snt.Linear(1)(pose_fea)

    # Back propagation or not ?
    if self._stop_grad_caps_inpt:
      input_pose = tf.stop_gradient(input_pose)
      input_pres = tf.stop_gradient(pres)

    target_pose, target_pres = pose, pres
    if self._stop_grad_caps_target:
      target_pose = tf.stop_gradient(target_pose)
      target_pres = tf.stop_gradient(target_pres)

    # skip connection from the img to the higher level capsule
    if primary_caps.feature is not None:
      input_pose = tf.concat([input_pose, primary_caps.feature], -1)

    # To get the templates from Part Capsule Decoder
    n_templates = int(primary_caps.pose.shape[1])
    templates = self._primary_decoder.make_templates(n_templates,
                                                     primary_caps.feature)

    try:
      if self._feed_templates:
        inpt_templates = templates
        if self._stop_grad_caps_inpt:
          inpt_templates = tf.stop_gradient(inpt_templates)

        if inpt_templates.shape[0] == 1:
          inpt_templates = snt.TileByDim([0], [batch_size])(inpt_templates)
        inpt_templates = snt.BatchFlatten(2)(inpt_templates)
        pose_with_templates = tf.concat([input_pose, inpt_templates], -1)
      else:
        pose_with_templates = input_pose

      #OCAE encoder part
      h = self._encoder(pose_with_templates, input_pres)

    except TypeError:
      h = self._encoder(input_pose)

    res, caps_presence_prob = self._decoder(h, target_pose, target_pres)
    res.primary_presence = primary_caps.presence

    if self._vote_type == 'enc':
      primary_dec_vote = primary_caps.pose
    elif self._vote_type == 'soft':
      primary_dec_vote = res.soft_winner
    elif self._vote_type == 'hard':
      primary_dec_vote = res.winner
    else:
      raise ValueError('Invalid vote_type="{}"".'.format(self._vote_type))

    if self._pres_type == 'enc':
      primary_dec_pres = pres
    elif self._pres_type == 'soft':
      primary_dec_pres = res.soft_winner_pres
    elif self._pres_type == 'hard':
      primary_dec_pres = res.winner_pres
    else:
      raise ValueError('Invalid pres_type="{}"".'.format(self._pres_type))

    rec = self._primary_decoder(
      primary_dec_vote,
      primary_dec_pres,
      template_feature=primary_caps.feature,
      img_embedding=primary_caps.img_embedding)

    res.templates = templates
    res.template_pres = pres
    res.used_templates = rec.transformed_templates

    res.rec_mode = rec.pdf.mode()
    res.rec_mean = rec.pdf.mean()

    rec_mode = tf.reshape(res.rec_mode, [batch_size, 256, 1])

    rec_mode = tf.transpose(rec_mode, [0, 2, 1])
    input_x = tf.transpose(tf.reshape(input_x, [1, batch_size, 256]), [1, 0, 2])

    res.mse_per_pixel = tf.square(input_x - rec_mode)
    rec_loss = tf.reduce_mean(input_x - rec_mode)
    res.mse = geo_block.flat_reduce(res.mse_per_pixel)

    res.rec_ll_per_pixel = rec.pdf.log_prob(input_x) # log likelihood for image
    res.rec_ll = geo_block.flat_reduce(res.rec_ll_per_pixel)

    gen_ll = rec.pdf.log_prob(rec_mode) # log likelihood for image
    gen_ll = geo_block.flat_reduce(gen_ll)

    n_points = int(res.posterior_mixing_probs.shape[1])
    mass_explained_by_capsule = tf.reduce_sum(res.posterior_mixing_probs,
                                              1, name = 'mass_explained_by_capsule')  # v_k

    posterior_pre = tf.squeeze(snt.BatchApply(snt.Linear(1, use_bias = False))(res.soft_winner), -1)
    posterior_pre = tf.multiply(posterior_pre, res.soft_winner_pres)
    posterior_pre = tf.nn.relu(snt.Linear(1)(posterior_pre))


    (res.posterior_within_sparsity_loss,
     res.posterior_between_sparsity_loss) = _capsule.sparsity_loss(
      self._posterior_sparsity_loss_type,
      mass_explained_by_capsule / n_points,
      num_classes=self._n_classes)

    (res.prior_within_sparsity_loss,
     res.prior_between_sparsity_loss) = _capsule.sparsity_loss(
      self._prior_sparsity_loss_type,
      res.caps_presence_prob,
      num_classes=self._n_classes,
      within_example_constant=self._prior_within_example_constant)


    res.posterior_pre_xe, posterior_logits, posterior_logits_1 = pred_loss(mass_explained_by_capsule, real_bg)

    res.prior_pre_xe, prior_logits, prior_logits_1 = pred_loss(res.caps_presence_prob, real_bg)

    best_pre_loss = tf.minimum(res.prior_pre_xe, res.posterior_pre_xe, name = 'best_pre_loss')

    res.primary_caps_l1 = geo_block.flat_reduce(res.primary_presence)
    #mse = pred_loss_1(res.prediction, real_bg)
    mse = pred_loss_1(dyn_pre, real_bg)
    res.scae_input = input_x

    if self._weight_decay > 0.0:
      decay_losses_list = []
      for var in tf.trainable_variables():
        if 'w:' in var.name or 'weights:' in var.name:
          decay_losses_list.append(tf.nn.l2_loss(var))
      res.weight_decay_loss = tf.reduce_sum(decay_losses_list)
    else:
      a = 0.0
      res.weight_decay_loss = tf.convert_to_tensor(a)

    epsilon = 1e-4
    real_loss = -(tf.reduce_mean(tf.log(res.rec_ll + epsilon) + tf.log(1 - gen_ll + epsilon)))
    gen_loss = -tf.reduce_mean(tf.log(gen_ll + epsilon))

    #vamp_autograd =  self.vamp.loss_VAMP2_autograd(tf.concat((input_x, rec_mode), axis = -1))
    #vamp_sym = self.vamp._loss_VAMP_sym(tf.concat((input_x, rec_mode), axis = -1))

    phi = 0.1
    loss = (- 0.1 * res.rec_ll -   1 * res.log_prob + 10 * res.dynamic_weights_l2 +  # 10
            0.5 * res.primary_caps_l1 + 10 *  res.posterior_within_sparsity_loss -
            10 * res.posterior_between_sparsity_loss +
            1 * res.prior_within_sparsity_loss - 1 * res.prior_between_sparsity_loss
            )
    log_loss = phi * loss

    likelihood_loss = tf.stack([log_loss,      mse], name = 'likelihood_loss')
    mse_loss        = tf.stack([mse,           mse], name = 'mse_loss')
    rec_mse_loss    = tf.stack([res.mse,       mse], name = 'rec_mse_loss')
    #vamp_aut_loss   = tf.stack([vamp_autograd, mse], name = 'vamp_aut_loss')
    #vamp_sym_loss   = tf.stack([vamp_sym,      mse], name = 'vamp_sym')

    return likelihood_loss, mse_loss, rec_mse_loss, dyn_pre


  def _likelihood_loss(self, data, res):
      likelihood_loss, mse = tf.unstack(res)

      loss = likelihood_loss

      phi = 0.1
      try:
          loss = phi * loss
          # loss += posterior_pre_xe + prior_pre_xe
      except AttributeError:
          pass

      return loss

  def _mse_loss(self, data, res):
      rec_mse, mse = tf.unstack(res)

      alpha = 1
      loss = alpha * mse
      return loss

  def _rec_mse_loss(self, data, res):
      rec_mse, mse = tf.unstack(res)

      beta = 1
      loss = beta * rec_mse
      return loss

  def _vamp_aut_loss(self, data, res):
      vamp_aut_loss, mse = tf.unstack(res)

      gamma = 1
      loss = gamma * vamp_aut_loss
      return loss

  def _vamp_sym_loss(self, data, res):
      vamp_sym_loss, mse = tf.unstack(res)

      lambd = 1
      loss = lambd * vamp_sym_loss
      return loss









